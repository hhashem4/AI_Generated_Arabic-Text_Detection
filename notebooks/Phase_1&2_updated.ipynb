{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "91e227c4",
      "metadata": {
        "id": "91e227c4"
      },
      "source": [
        "## Task 1.2: Use the datasets library from Hugging Face to download the arabic- generated-abstracts dataset directly into a Python environment (By Google Colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "792b9e4b",
      "metadata": {
        "id": "792b9e4b"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "# !pip install python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be1008dc",
      "metadata": {
        "id": "be1008dc"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dea3272",
      "metadata": {
        "id": "5dea3272"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "dataset = load_dataset(\"KFUPM-JRCAI/arabic-generated-abstracts\")\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print split names\n",
        "print(\"\\nAvailable splits:\", list(dataset.keys()))"
      ],
      "metadata": {
        "id": "Obe9S8U9hqTc"
      },
      "id": "Obe9S8U9hqTc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "997695cc",
      "metadata": {
        "id": "997695cc"
      },
      "source": [
        "### Task 1.3: Perform initial data exploration:\n",
        "\n",
        "\n",
        "\n",
        "#### Explore Any Split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def explore_split(dataset, split_name):\n",
        "    print(f\"\\n\\n==============================================\")\n",
        "    print(f\" Exploring Split: {split_name}\")\n",
        "    print(\"==============================================\")\n",
        "\n",
        "    split = dataset[split_name]\n",
        "\n",
        "    # Inspect features\n",
        "    print(\"\\n➡ Column Names and Data Types:\")\n",
        "    print(split.features)\n",
        "\n",
        "    # Convert to pandas\n",
        "    df = split.to_pandas()\n",
        "\n",
        "    print(\"\\n➡ First 5 Rows:\")\n",
        "    display(df.head())\n",
        "\n",
        "    print(\"\\n➡ Dataset Shape:\")\n",
        "    print(df.shape)\n",
        "\n",
        "    print(\"\\n➡ Columns and Types:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # ---- Target variable check ----\n",
        "    if \"label\" in df.columns:\n",
        "        print(\"\\n➡ Target Variable Distribution (0 = Human, 1 = AI Generated):\")\n",
        "        print(df[\"label\"].value_counts())\n",
        "\n",
        "        print(\"\\n➡ Percentage Distribution (%):\")\n",
        "        print(df[\"label\"].value_counts(normalize=True) * 100)\n",
        "    else:\n",
        "        print(\"\\n⚠ No 'label' column found — skipping label distribution.\")\n",
        "\n",
        "    # ---- Text length analysis ----\n",
        "    text_columns = [c for c in df.columns if c != \"label\"]\n",
        "\n",
        "    if text_columns:\n",
        "        text_col = text_columns[0]\n",
        "        df[\"text_length\"] = df[text_col].astype(str).apply(len)\n",
        "\n",
        "        print(f\"\\n➡ Text Length Summary for column: {text_col}\")\n",
        "        print(df[\"text_length\"].describe())\n",
        "\n",
        "        print(f\"\\n➡ Sample Text Example from {text_col}:\")\n",
        "        print(\"\\n\", df[text_col].iloc[0])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "3s-s7NhtZadN"
      },
      "id": "3s-s7NhtZadN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1452b10e",
      "metadata": {
        "id": "1452b10e"
      },
      "outputs": [],
      "source": [
        "splits = [\"by_polishing\", \"from_title\", \"from_title_and_content\"]\n",
        "\n",
        "dfs = {}  # store pandas dataframes\n",
        "\n",
        "for split in splits:\n",
        "    dfs[split] = explore_split(dataset, split)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3afb712",
      "metadata": {
        "id": "c3afb712"
      },
      "source": [
        "#### 2- Check the distribution of the target variable (label: human vs. AI)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "477d5b06",
      "metadata": {
        "id": "477d5b06"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a289922",
      "metadata": {
        "id": "5a289922"
      },
      "outputs": [],
      "source": [
        "# Function to compute distribution for any split\n",
        "def compute_distribution(split, split_name):\n",
        "    print(f\"\\n===== Distribution for split: {split_name} =====\")\n",
        "\n",
        "    # Count human abstracts (1 per row)\n",
        "    num_human = len(split[\"original_abstract\"])\n",
        "\n",
        "    # Count AI abstracts (4 per row)\n",
        "    num_ai = (\n",
        "        len(split[\"allam_generated_abstract\"])\n",
        "        + len(split[\"jais_generated_abstract\"])\n",
        "        + len(split[\"llama_generated_abstract\"])\n",
        "        + len(split[\"openai_generated_abstract\"])\n",
        "    )\n",
        "\n",
        "    # Print raw counts\n",
        "    print(\"Number of human abstracts:\", num_human)\n",
        "    print(\"Number of AI-generated abstracts:\", num_ai)\n",
        "\n",
        "    # Percentages\n",
        "    total = num_human + num_ai\n",
        "    if total > 0:\n",
        "        print(\"Human %:\", round(num_human / total * 100, 2))\n",
        "        print(\"AI %:\", round(num_ai / total * 100, 2))\n",
        "    else:\n",
        "        print(\"No data available.\")\n",
        "\n",
        "\n",
        "# Apply to the 3 main splits\n",
        "compute_distribution(dataset[\"by_polishing\"], \"by_polishing\")\n",
        "compute_distribution(dataset[\"from_title\"], \"from_title\")\n",
        "compute_distribution(dataset[\"from_title_and_content\"], \"from_title_and_content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "079c4cb2",
      "metadata": {
        "id": "079c4cb2"
      },
      "source": [
        "#### 3- Assess data quality: check for missing values, duplicates, and inconsistencies:\n",
        "\n",
        "\n",
        "Missing values → any None/NaN in columns\n",
        "\n",
        "Duplicates → same abstract appearing multiple times\n",
        "\n",
        "Inconsistencies → like empty strings \" \" or unusual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbce6d3d",
      "metadata": {
        "id": "fbce6d3d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "splits = [\"by_polishing\", \"from_title\", \"from_title_and_content\"]\n",
        "\n",
        "for split_name in splits:\n",
        "    print(\"\\n========================================\")\n",
        "    print(f\"Data Quality Checks — Split: {split_name}\")\n",
        "    print(\"========================================\\n\")\n",
        "\n",
        "    split = dataset[split_name]\n",
        "\n",
        "    # Convert to pandas DataFrame\n",
        "    df = split.to_pandas()\n",
        "\n",
        "    # 1. Missing values\n",
        "    print(\" Missing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    # 2. Duplicate rows\n",
        "    total_duplicates = df.duplicated().sum()\n",
        "    print(f\" Number of duplicate rows: {total_duplicates}\")\n",
        "\n",
        "    # Duplicates in each column\n",
        "    for col in df.columns:\n",
        "        col_duplicates = df[col].duplicated().sum()\n",
        "        print(f\"  Duplicates in column '{col}': {col_duplicates}\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    # 3. Inconsistencies: empty strings or only spaces\n",
        "    print(\" Empty or blank values per column:\")\n",
        "    for col in df.columns:\n",
        "        empty_count = df[col].apply(lambda x: str(x).strip() == \"\").sum()\n",
        "        print(f\"  Column '{col}': {empty_count}\")\n",
        "\n",
        "    print(\"\\n\\n\")  # space between splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e825541a",
      "metadata": {
        "id": "e825541a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tEON2s3Ql-tP"
      },
      "id": "tEON2s3Ql-tP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2 -preprocessing"
      ],
      "metadata": {
        "id": "Jx_JLBu23w_x"
      },
      "id": "Jx_JLBu23w_x"
    },
    {
      "cell_type": "code",
      "source": [
        "# task 2.1: Arabic Text Preprocessing\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "cINVvDvxo1sY"
      },
      "id": "cINVvDvxo1sY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "dge6SB2mp1kx"
      },
      "id": "dge6SB2mp1kx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check columns\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "xTeRHx-rp43g"
      },
      "id": "xTeRHx-rp43g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Arabic text cleaning functions"
      ],
      "metadata": {
        "id": "fTPi6B6V5H6K"
      },
      "id": "fTPi6B6V5H6K"
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove tashkeel (diacritics)\n",
        "def remove_diacritics(text):\n",
        "    arabic_diacritics = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    return re.sub(arabic_diacritics, '', text)"
      ],
      "metadata": {
        "id": "xvK4KAkWqNt-"
      },
      "id": "xvK4KAkWqNt-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize Arabic text\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"و\", text)\n",
        "    text = re.sub(\"ئ\", \"ي\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"[^؀-ۿ ]+\", \" \", text)  # remove non-Arabic chars\n",
        "    return text"
      ],
      "metadata": {
        "id": "v_IZB2y2q9Ip"
      },
      "id": "v_IZB2y2q9Ip",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize stopwords and stemmer\n",
        "arabic_stopwords = set(stopwords.words(\"arabic\"))\n",
        "stemmer = ISRIStemmer()"
      ],
      "metadata": {
        "id": "BuV9q2b0rCtW"
      },
      "id": "BuV9q2b0rCtW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full preprocessing pipeline\n",
        "def preprocess_text(text):\n",
        "    text = str(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = normalize_arabic(text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in arabic_stopwords]\n",
        "    tokens = [stemmer.stem(w) for w in tokens]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "8qhJ9wWVrFZV"
      },
      "id": "8qhJ9wWVrFZV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "text_columns = [\n",
        "    'original_abstract',\n",
        "    'allam_generated_abstract',\n",
        "    'jais_generated_abstract',\n",
        "    'llama_generated_abstract',\n",
        "    'openai_generated_abstract'\n",
        "]\n",
        "for col in text_columns:\n",
        "    clean_col = col + \"_clean\"\n",
        "    df[clean_col] = df[col].apply(preprocess_text)\n",
        "print(\" Preprocessing complete! Here are the new columns:\")\n",
        "print(df.columns)\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "VDEQ2M1RrOW3"
      },
      "id": "VDEQ2M1RrOW3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Task 2.2: Exploratory Data Analysis (EDA)\n",
        "\n"
      ],
      "metadata": {
        "id": "pn9LXPqr8w0R"
      },
      "id": "pn9LXPqr8w0R"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "aoZRcVsRrQP_"
      },
      "id": "aoZRcVsRrQP_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Length Distribution Plot\n",
        "\n"
      ],
      "metadata": {
        "id": "zZBc802WjijV"
      },
      "id": "zZBc802WjijV"
    },
    {
      "cell_type": "code",
      "source": [
        "for split, df in dfs.items():\n",
        "    text_columns = [c for c in df.columns if c != \"label\"]\n",
        "    if not text_columns:\n",
        "        continue\n",
        "\n",
        "    text_col = text_columns[0]\n",
        "\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    df[\"text_length\"].hist(bins=40)\n",
        "    plt.title(f\"Text Length Distribution — {split}\")\n",
        "    plt.xlabel(\"Length (characters)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mjz_3_FEjla6"
      },
      "id": "mjz_3_FEjla6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Length per Label"
      ],
      "metadata": {
        "id": "mnkMMC4bkXem"
      },
      "id": "mnkMMC4bkXem"
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a split\n",
        "split_name = \"by_polishing\"\n",
        "df = dataset[split_name].to_pandas()\n",
        "\n",
        "# Identify text and label columns\n",
        "text_col = [c for c in df.columns if \"abstract\" in c.lower()][0]  # pick first abstract column\n",
        "label_col = \"label\" if \"label\" in df.columns else None\n",
        "\n",
        "# Compute text length (characters)\n",
        "df[\"text_length\"] = df[text_col].astype(str).apply(len)\n",
        "\n",
        "# Describe overall\n",
        "print(\"Overall text length statistics:\")\n",
        "print(df[\"text_length\"].describe())\n",
        "\n",
        "# Text length per label\n",
        "if label_col:\n",
        "    print(\"\\nText length stats per label:\")\n",
        "    print(df.groupby(label_col)[\"text_length\"].describe())"
      ],
      "metadata": {
        "id": "SLDe6dqdkMu7"
      },
      "id": "SLDe6dqdkMu7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram for text length\n",
        "plt.figure(figsize=(7, 4))\n",
        "df[\"text_length\"].hist(bins=50, color=\"skyblue\")\n",
        "plt.title(f\"Text Length Distribution — {split_name}\")\n",
        "plt.xlabel(\"Characters\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Plot per label if exists\n",
        "if label_col:\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    for lbl in df[label_col].unique():\n",
        "        subset = df[df[label_col]==lbl]\n",
        "        plt.hist(subset[\"text_length\"], bins=50, alpha=0.6, label=f\"Label {lbl}\")\n",
        "    plt.title(f\"Text Length Distribution by Label — {split_name}\")\n",
        "    plt.xlabel(\"Characters\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ORmaLXzujkha"
      },
      "id": "ORmaLXzujkha",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word Count per Text"
      ],
      "metadata": {
        "id": "zk_jWuowkoYK"
      },
      "id": "zk_jWuowkoYK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Word count\n",
        "df[\"word_count\"] = df[text_col].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "print(\"Word count statistics overall:\")\n",
        "print(df[\"word_count\"].describe())\n",
        "\n",
        "if label_col:\n",
        "    print(\"\\nWord count stats per label:\")\n",
        "    print(df.groupby(label_col)[\"word_count\"].describe())"
      ],
      "metadata": {
        "id": "2hOelXMXjkkz"
      },
      "id": "2hOelXMXjkkz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Overall\n",
        "all_words = \" \".join(df[text_col].astype(str)).split()\n",
        "most_common = Counter(all_words).most_common(20)\n",
        "print(\"\\nMost common words overall:\")\n",
        "print(most_common)\n",
        "\n",
        "# Per label\n",
        "if label_col:\n",
        "    for lbl in df[label_col].unique():\n",
        "        words = \" \".join(df[df[label_col]==lbl][text_col].astype(str)).split()\n",
        "        most_common_lbl = Counter(words).most_common(15)\n",
        "        print(f\"\\nMost common words for label {lbl}:\")\n",
        "        print(most_common_lbl)"
      ],
      "metadata": {
        "id": "SQnRv3D-jkoo"
      },
      "id": "SQnRv3D-jkoo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZX6XpBtP3hP9"
      },
      "id": "ZX6XpBtP3hP9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Top 20 Most Frequent Words"
      ],
      "metadata": {
        "id": "iNEj6pwdmm2N"
      },
      "id": "iNEj6pwdmm2N"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "for split_name in splits:\n",
        "    df = dataset[split_name].to_pandas()\n",
        "    text_col = [c for c in df.columns if \"abstract\" in c.lower()][0]\n",
        "\n",
        "    all_words = \" \".join(df[text_col].astype(str)).split()\n",
        "    most_common = Counter(all_words).most_common(20)\n",
        "\n",
        "    words, counts = zip(*most_common)\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.bar(words, counts, color=\"skyblue\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.title(f\"Top 20 Most Frequent Words — {split_name}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QklOCVNb4WQS"
      },
      "id": "QklOCVNb4WQS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HMZzWtam3FVa"
      },
      "id": "HMZzWtam3FVa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_DIN97bNtrXI"
      },
      "id": "_DIN97bNtrXI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tGrrucpFms0L"
      },
      "id": "tGrrucpFms0L"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "master_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}